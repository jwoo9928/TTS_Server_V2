# main.py
import asyncio
import io
import numpy as np
import soundfile as sf
import torch
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from contextlib import asynccontextmanager
from kokoro import KPipeline
import logging
import os
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Determine device based on CUDA availability
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

# Configuration
NUM_PIPELINES = int(os.getenv("NUM_PIPELINES", min(os.cpu_count() or 1, 3)))
logger.info(f"Initializing {NUM_PIPELINES} pipeline instances")

# Pipeline pool management
class PipelinePool:
    def __init__(self, num_pipelines=3, lang_code='a', device=device):
        self.pipelines = []
        self.locks = []
        self.available = []
        self.pool_lock = Lock()
        
        for i in range(num_pipelines):
            logger.info(f"Initializing pipeline {i+1}/{num_pipelines}")
            pipeline = KPipeline(lang_code=lang_code, device=device)
            self.pipelines.append(pipeline)
            self.locks.append(Lock())
            self.available.append(True)
        
        logger.info(f"Pipeline pool initialized with {num_pipelines} instances")
    
    async def get_pipeline(self):
        """Get an available pipeline from the pool"""
        while True:
            with self.pool_lock:
                for i, available in enumerate(self.available):
                    if available:
                        self.available[i] = False
                        return i
            
            # All pipelines are busy, wait a bit and retry
            await asyncio.sleep(0.1)
    
    def release_pipeline(self, index):
        """Release a pipeline back to the pool"""
        with self.pool_lock:
            self.available[index] = True
    
    def generate_audio(self, pipeline_index, text, voice, speed, split_pattern):
        """Generate audio using a specific pipeline"""
        pipeline = self.pipelines[pipeline_index]
        
        try:
            logger.info(f"Generating audio with pipeline {pipeline_index} for voice: {voice}")
            generator = pipeline(text, voice=voice, speed=speed, split_pattern=split_pattern)
            
            all_audio_chunks = []
            for i, (gs, ps, audio_chunk) in enumerate(generator):
                logger.debug(f"Generated chunk {i} with pipeline {pipeline_index}")
                all_audio_chunks.append(audio_chunk)
            
            if not all_audio_chunks:
                logger.warning(f"No audio chunks generated by pipeline {pipeline_index}")
                return None

            # Concatenate chunks
            if isinstance(all_audio_chunks[0], torch.Tensor):
                full_audio = torch.cat(all_audio_chunks)
                full_audio_np = full_audio.cpu().numpy()
            else:
                full_audio_np = np.concatenate(all_audio_chunks)

            # Save to an in-memory WAV file
            buffer = io.BytesIO()
            sf.write(buffer, full_audio_np, 24000, format='WAV')
            buffer.seek(0)
            logger.info(f"Audio generation complete with pipeline {pipeline_index}")
            return buffer
        except Exception as e:
            logger.error(f"Error during audio generation with pipeline {pipeline_index}: {e}", exc_info=True)
            raise

# Global pool variable
pipeline_pool = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Load the model during startup
    global pipeline_pool
    logger.info(f"Initializing pipeline pool with {NUM_PIPELINES} instances")
    try:
        pipeline_pool = PipelinePool(num_pipelines=NUM_PIPELINES, lang_code='a', device=device)
        logger.info("Pipeline pool loaded successfully")
    except Exception as e:
        logger.error(f"Failed to initialize pipeline pool: {e}", exc_info=True)
        pipeline_pool = None
    yield
    # Clean up resources if needed during shutdown
    logger.info("Shutting down...")
    pipeline_pool = None  # Release memory

app = FastAPI(lifespan=lifespan)

class TTSRequest(BaseModel):
    text: str
    voice: str = 'af_heart'  # Default voice
    speed: float = 1.0
    split_pattern: str = r'\n+'

@app.post("/tts")
async def text_to_speech(request_data: TTSRequest):
    """
    Generates TTS audio and returns the full WAV file.
    Handles requests concurrently using pipeline pool.
    """
    if pipeline_pool is None:
        raise HTTPException(status_code=503, detail="TTS Service Unavailable: Model not loaded.")

    try:
        # Get an available pipeline from the pool
        pipeline_index = await pipeline_pool.get_pipeline()
        logger.info(f"Using pipeline {pipeline_index} for request")
        
        try:
            # Run the audio generation in a thread
            loop = asyncio.get_running_loop()
            audio_buffer = await loop.run_in_executor(
                None,
                lambda: pipeline_pool.generate_audio(
                    pipeline_index,
                    request_data.text,
                    request_data.voice,
                    request_data.speed,
                    request_data.split_pattern
                )
            )
        finally:
            # Always release the pipeline back to the pool
            pipeline_pool.release_pipeline(pipeline_index)
        
        if audio_buffer is None:
            raise HTTPException(status_code=500, detail="TTS generation failed: No audio produced.")

        # Return the audio stream
        return StreamingResponse(
            audio_buffer, 
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=audio.wav"}
        )
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=f"TTS Service Unavailable: {str(e)}")
    except Exception as e:
        logger.error(f"Unhandled exception in /tts endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

@app.post("/tts-stream")
async def text_to_speech_stream(request_data: TTSRequest):
    """
    Generates TTS audio and streams the WAV audio chunks as they become available.
    """
    if pipeline_pool is None:
        raise HTTPException(status_code=503, detail="TTS Service Unavailable: Model not loaded.")

    # Get an available pipeline from the pool
    pipeline_index = None
    
    async def audio_stream_generator():
        nonlocal pipeline_index
        
        try:
            # Acquire a pipeline
            pipeline_index = await pipeline_pool.get_pipeline()
            logger.info(f"Using pipeline {pipeline_index} for streaming request")
            
            pipeline = pipeline_pool.pipelines[pipeline_index]
            
            # Get the generator from the pipeline
            generator = pipeline(
                request_data.text, 
                voice=request_data.voice, 
                speed=request_data.speed, 
                split_pattern=request_data.split_pattern
            )
            
            # Send WAV header first
            yield b'RIFF\0\0\0\0WAVEfmt \x10\0\0\0\x01\0\x01\0\x80\\\0\0\0\x01\x18\0data\0\0\0\0'
            
            # Process each chunk
            for i, (gs, ps, audio_chunk) in enumerate(generator):
                logger.debug(f"Streaming chunk {i} with pipeline {pipeline_index}")
                # Convert tensor to numpy if needed
                if isinstance(audio_chunk, torch.Tensor):
                    audio_np = audio_chunk.cpu().numpy()
                else:
                    audio_np = audio_chunk
                    
                # Convert to bytes and yield
                audio_bytes = audio_np.tobytes()
                yield audio_bytes
                
            logger.info(f"Streaming generation complete with pipeline {pipeline_index}")
            
        except Exception as e:
            logger.error(f"Error during streaming audio generation: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Streaming Error: {str(e)}")
        finally:
            # Always release the pipeline back to the pool
            if pipeline_index is not None:
                pipeline_pool.release_pipeline(pipeline_index)

    try:
        return StreamingResponse(
            audio_stream_generator(),
            media_type="audio/wav",
        )
    except Exception as e:
        logger.error(f"Unhandled exception in /tts-stream endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)